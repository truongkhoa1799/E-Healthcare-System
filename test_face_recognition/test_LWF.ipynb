{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST FACE RECOGNITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "LWF_FOLDER_FACE = '/Users/khoa1799/Documents/DATA/LWF/data'\n",
    "LWF_CUT_FOLDER_FACE = '/Users/khoa1799/Documents/DATA/LWF/cut_data'\n",
    "\n",
    "LWF_PATH_USER_ID = '/Users/khoa1799/Documents/E-Healthcare-System/test_face_recognition/model/LWF_ID_Face'\n",
    "LWF_PATH_USER_IMG_ENCODED = '/Users/khoa1799/Documents/E-Healthcare-System/test_face_recognition/model/LWF_Encoded_Face'\n",
    "LWF_KNN_MODEL_PATH = \"/Users/khoa1799/Documents/E-Healthcare-System/test_face_recognition/model/LWF_knn_clf_model.clf\"\n",
    "LWF_SVM_MODEL_PATH = \"/Users/khoa1799/Documents/E-Healthcare-System/test_face_recognition/model/LWF_svm_clf_model.clf\"\n",
    "\n",
    "PREDICTOR_5_POINT_MODEL = '/Users/khoa1799/Documents/E-Healthcare-System/test_face_recognition/model/shape_predictor_5_face_landmarks.dat'\n",
    "RESNET_MODEL = '/Users/khoa1799/Documents/E-Healthcare-System/test_face_recognition/model/dlib_face_recognition_resnet_model_v1.dat'\n",
    "\n",
    "IMAGE_SIZE = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FACE ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "pose_predictor_5_point = dlib.shape_predictor(PREDICTOR_5_POINT_MODEL)\n",
    "face_encoder = dlib.face_recognition_model_v1(RESNET_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 250, 3)\n"
     ]
    }
   ],
   "source": [
    "def face_encodings(face_image, known_face_locations):\n",
    "    raw_landmarks = _raw_face_landmarks(face_image, known_face_locations)\n",
    "    return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, 1)) for raw_landmark_set in raw_landmarks]\n",
    "\n",
    "def _css_to_rect(css):\n",
    "    return dlib.rectangle(css[3], css[0], css[1], css[2])\n",
    "\n",
    "def _raw_face_landmarks(face_image, face_locations):\n",
    "    if face_locations is None:\n",
    "        face_locations = _raw_face_locations(face_image)\n",
    "    else:\n",
    "        face_locations = [_css_to_rect(face_location) for face_location in face_locations]\n",
    "\n",
    "    pose_predictor = pose_predictor_5_point\n",
    "\n",
    "    return [pose_predictor(face_image, face_location) for face_location in face_locations]\n",
    "\n",
    "import cv2\n",
    "img = cv2.imread(LWF_FOLDER_FACE+ '/Donald_Fehr'+'/Donald_Fehr_0004.jpg')\n",
    "print(img.shape)\n",
    "test = face_encodings(img, [(0,250,250,0)])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET THE BOUNDING BOX CONTAINNING FACE IN IMAGE BUT NOT ALIGNED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cut_Img(loaded_img):\n",
    "    ret_img = loaded_img.copy()\n",
    "    \n",
    "    max_fra = max( loaded_img.shape[0], loaded_img.shape[1] ) / 320\n",
    "    new_height = int( loaded_img.shape[0] / max_fra )\n",
    "    new_width = int( loaded_img.shape[1] / max_fra )\n",
    "\n",
    "    resized_img = cv2.resize(loaded_img, ( new_width, new_height ))\n",
    "    RGB_resized_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    face_locations = face_recognition.face_locations(RGB_resized_img)\n",
    "    for (top, right, bottom, left) in face_locations:\n",
    "        top *= max_fra\n",
    "        bottom *= max_fra\n",
    "        left *= max_fra\n",
    "        right *= max_fra\n",
    "\n",
    "        return len(face_locations), ret_img[int(top):int(bottom),int(left):int(right)]\n",
    "    return 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD AND SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveData(known_face_IDs, known_face_encodings, path_user_id, path_user_img_encoded):\n",
    "    with open(path_user_id, mode='wb') as fp_1:\n",
    "        pickle.dump(known_face_IDs, fp_1)\n",
    "\n",
    "    with open(path_user_img_encoded, 'wb') as fp_2:\n",
    "        pickle.dump(known_face_encodings, fp_2)\n",
    "\n",
    "def SaveKNNModel(knn_clf, path_model):\n",
    "    if path_model is not None:\n",
    "        with open(path_model, 'wb') as f:\n",
    "            pickle.dump(knn_clf, f)\n",
    "\n",
    "def SaveSVMModel(svm_clf, path_model):\n",
    "    if path_model is not None:\n",
    "        with open(path_model, 'wb') as f:\n",
    "            pickle.dump(svm_clf, f)\n",
    "\n",
    "def LoadData(path_user_id, path_user_img_encoded):\n",
    "    known_face_IDs = None\n",
    "    known_face_encodings = None\n",
    "    if not os.path.exists(path_user_id):\n",
    "        print(\"There is no user id face to load\")\n",
    "        exit(-1)\n",
    "\n",
    "    with open (path_user_id, 'rb') as fp_1:\n",
    "        known_face_IDs = pickle.load(fp_1)\n",
    "\n",
    "    if not os.path.exists(path_user_img_encoded):\n",
    "        print(\"There is no user encoding face to load\")\n",
    "        exit(-1)\n",
    "\n",
    "    with open (path_user_img_encoded, 'rb') as fp_2:\n",
    "        known_face_encodings = pickle.load(fp_2)\n",
    "    \n",
    "    return known_face_IDs, known_face_encodings\n",
    "        \n",
    "def LoadKNNModel(path_model):\n",
    "    if not os.path.exists(path_model):\n",
    "        print(\"There is KNN model to load\")\n",
    "        exit(-1)\n",
    "\n",
    "    with open(path_model, 'rb') as f:\n",
    "        knn_clf = pickle.load(f)\n",
    "        return knn_clf\n",
    "\n",
    "def LoadSVMModel(path_model):\n",
    "    if not os.path.exists(SVM_MODEL_PATH):\n",
    "        print(\"There is KNN model to load\")\n",
    "        exit(-1)\n",
    "\n",
    "    with open(SVM_MODEL_PATH, 'rb') as f:\n",
    "        svm_clf = pickle.load(f)\n",
    "        return svm_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LWF DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "BASE_BRIGHTNESS = 180\n",
    "IMAGE_SIZE = 150\n",
    "\n",
    "def adjust_brightness(img):\n",
    "    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) #convert it to hsv\n",
    "    v = hsv_img[:, :, 2]\n",
    "    mean_v = np.mean(v)\n",
    "    diff = BASE_BRIGHTNESS - mean_v\n",
    "                   \n",
    "    if diff < 0:\n",
    "        v = np.where(v < abs(diff), v, v + diff)\n",
    "    else:\n",
    "        v = np.where( v + diff > 255, v, v + diff)\n",
    "\n",
    "    hsv_img[:, :, 2] = v\n",
    "    ret_img = cv2.cvtColor(hsv_img, cv2.COLOR_HSV2BGR)\n",
    "    # return BRG image\n",
    "    return ret_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST LWF DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw_people = fetch_lfw_people(min_faces_per_person=5, resize=1, color=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test and train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images: 4488\n",
      "Number of test images: 1497\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List BGR image\n",
    "X = lfw_people.images\n",
    "X = [cv2.normalize(src=x, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)for x in X]\n",
    "X = [cv2.cvtColor(x, cv2.COLOR_RGB2BGR) for x in X]\n",
    "\n",
    "Y = lfw_people.target\n",
    "Y = [y for y in Y]\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=20)\n",
    "print(\"Number of train images: {}\".format(len(X_train)))\n",
    "print(\"Number of test images: {}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of each images: (125, 94, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of each images: {}\".format(X[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374\n"
     ]
    }
   ],
   "source": [
    "freq = {}\n",
    "max_people = 0\n",
    "return_face_id = None\n",
    "for user_id in y_test: \n",
    "    if (user_id in freq): \n",
    "        freq[user_id] += 1\n",
    "    else: \n",
    "        freq[user_id] = 1\n",
    "        \n",
    "print(len(freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE EMBEDDED CODE AND USER ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time load data 102.51524925231934\n"
     ]
    }
   ],
   "source": [
    "known_face_IDs = []\n",
    "known_face_encodings = []\n",
    "IMAGE_SIZE = 150\n",
    "\n",
    "time_start = time.time()\n",
    "for i in range(len(X_train)):\n",
    "    img = X_train[i]\n",
    "    user_ID = y_train[i]\n",
    "    \n",
    "    resized_img = cv2.resize(img, (IMAGE_SIZE,IMAGE_SIZE))\n",
    "    resized_img = adjust_brightness(resized_img)\n",
    "    RGB_resized_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    pre_proc_img = RGB_resized_img\n",
    "    embedded_face = face_encodings(pre_proc_img, [(0,IMAGE_SIZE,IMAGE_SIZE,0)])[0]\n",
    "\n",
    "    known_face_encodings.append(embedded_face)\n",
    "    known_face_IDs.append(user_ID)\n",
    "\n",
    "SaveData(known_face_IDs, known_face_encodings, LWF_PATH_USER_ID, LWF_PATH_USER_IMG_ENCODED)\n",
    "print(\"Time load data {}\".format(time.time() - time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN KNN FOR LWF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing train KNN Model: 0.050067901611328125 second\n"
     ]
    }
   ],
   "source": [
    "# KNN PARAs\n",
    "NUM_NEIGHBROS = 5\n",
    "KNN_ALGORITHM = 'ball_tree'\n",
    "KNN_WEIGHTS = 'distance'\n",
    "\n",
    "knn_clf = None\n",
    "known_face_IDs = []\n",
    "known_face_encodings = []\n",
    "\n",
    "# Load data\n",
    "known_face_IDs, known_face_encodings = LoadData(LWF_PATH_USER_ID, LWF_PATH_USER_IMG_ENCODED)\n",
    "\n",
    "# Create and train the KNN classifier\n",
    "time_request = time.time()\n",
    "knn_clf = neighbors.KNeighborsClassifier(n_neighbors=NUM_NEIGHBROS, algorithm=KNN_ALGORITHM, weights=KNN_WEIGHTS, metric='euclidean')\n",
    "\n",
    "# self.__known_face_encodings is list of ndarray\n",
    "# self.__known_face_IDs is list of str\n",
    "knn_clf.fit(known_face_encodings, known_face_IDs)\n",
    "SaveKNNModel(knn_clf, LWF_KNN_MODEL_PATH)\n",
    "print(\"Finishing train KNN Model: {} second\".format(time.time() - time_request))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST FACE RECOGNITION FOR LWF DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean distance: 0.35181051751986525\n",
      "Max distance: 0.49969589316406\n",
      "Min distance: 0.0919298435723819\n",
      "\n",
      "Statistic report for identifying known face:\n",
      "\tAccurancy score: 0.949050949050949\n",
      "\tPrecision score: 0.9562814875039402\n",
      "\tRecall score: 0.949050949050949\n",
      "\tTotal images: 1001\n",
      "\tTotal time: 3.003488302230835\n",
      "\tMeans time: 0.0030004878144164185\n",
      "\tStandard deviation time: 0.00036678161859896616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khoa1799/.virtualenvs/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/khoa1799/.virtualenvs/my_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "THRESHOLD_FACE_REC = 0.5\n",
    "knn_clf = None\n",
    "known_face_IDs = None\n",
    "known_face_encodings = None\n",
    "\n",
    "dis_known_face = []\n",
    "predict_known_face = []\n",
    "list_time_known_faces = []\n",
    "total_faces = 0\n",
    "\n",
    "# Load KNN Model\n",
    "knn_clf = LoadKNNModel(LWF_KNN_MODEL_PATH)\n",
    "known_face_IDs, known_face_encodings = LoadData(LWF_PATH_USER_ID, LWF_PATH_USER_IMG_ENCODED)\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    img = X_test[i]\n",
    "    \n",
    "    # Pre-processing\n",
    "    resized_img = cv2.resize(img, (IMAGE_SIZE,IMAGE_SIZE))\n",
    "    resized_img = adjust_brightness(resized_img)\n",
    "    RGB_resized_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert to embedded code\n",
    "    pre_proc_img = RGB_resized_img\n",
    "    embedded_face = face_recognition.face_encodings(pre_proc_img, [(0,IMAGE_SIZE,IMAGE_SIZE,0)])[0]\n",
    "    embedded_face = embedded_face.reshape(1,-1)\n",
    "    \n",
    "    has_face = False\n",
    "    total_faces += 1\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Classifying\n",
    "    closet_distances = knn_clf.kneighbors(embedded_face, n_neighbors = 3)\n",
    "    face_id = knn_clf.predict(embedded_face)\n",
    "    meet_condition_threshold = [closet_distances[0][0][i] <= THRESHOLD_FACE_REC for i in range(len(closet_distances[0][0]))]\n",
    "    \n",
    "#     print(closet_distances)\n",
    "#     print(face_id[-1])\n",
    "    \n",
    "    for i in range(len(meet_condition_threshold)):\n",
    "        if meet_condition_threshold[i] and known_face_IDs[closet_distances[1][0][i]] == face_id[-1]:\n",
    "            predict_known_face.append(face_id[-1])\n",
    "            dis_known_face.append(closet_distances[0][0][i])\n",
    "            has_face = True\n",
    "            break\n",
    "    if has_face == False:\n",
    "        predict_known_face.append(-1)\n",
    "        \n",
    "    list_time_known_faces.append(time.time() - start_time)\n",
    "    \n",
    "    if total_faces > 1000:\n",
    "        break\n",
    "\n",
    "# print(predict_known_face)\n",
    "# print(y_test[0:total_faces])\n",
    "accurancy_score = accuracy_score(y_test[0:total_faces], predict_known_face)\n",
    "precision_score = precision_score(y_test[0:total_faces], predict_known_face, average='weighted')\n",
    "recall_score = recall_score(y_test[0:total_faces], predict_known_face, average='weighted')\n",
    "\n",
    "means = np.mean(list_time_known_faces)\n",
    "std = np.std(list_time_known_faces)\n",
    "total_time = np.sum(list_time_known_faces)\n",
    "\n",
    "print(\"Mean distance: {}\".format(np.mean(dis_known_face)))\n",
    "print(\"Max distance: {}\".format(np.max(dis_known_face)))\n",
    "print(\"Min distance: {}\".format(np.min(dis_known_face)))\n",
    "print()\n",
    "print(\"Statistic report for identifying known face:\")\n",
    "\n",
    "print(\"\\tAccurancy score: {}\".format(accurancy_score))\n",
    "print(\"\\tPrecision score: {}\".format(precision_score))\n",
    "print(\"\\tRecall score: {}\".format(recall_score))\n",
    "print()\n",
    "print(\"\\tTotal images: {}\".format(total_faces))\n",
    "print(\"\\tTotal time: {}\".format(total_time))\n",
    "print(\"\\tMeans time: {}\".format(means))\n",
    "print(\"\\tStandard deviation time: {}\".format(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
